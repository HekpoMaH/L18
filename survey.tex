\documentclass{article}

\usepackage{natbib}
\begin{document}

\nocite{SRASS}
\nocite{LightweightPaulson}
\nocite{MaSh}
\nocite{SNoW}
\nocite{MizarMode}
\nocite{DataMiningAFT}
\nocite{MaLARea}
\nocite{MaLARea04}
\nocite{DeepMath}
\nocite{FlyspeckFeatW}
\nocite{ProofGeneral}
\nocite{MLProofGeneral}
\nocite{MaLAReaSG}
\nocite{RLTP}
\nocite{PremiseSelection}
\nocite{DNGPS}
\nocite{malecop}
\nocite{femalecop}
\nocite{Holstep}
\nocite{EndToEndDiffProving}
\nocite{FeatureBasedThmP}
\nocite{OverviewEvalPremiseSel}
\nocite{LearnBasedFactSelIsabel}
\nocite{Flyspeck}
\nocite{Mizar40}

\cite{FuchsGenetic} -- heuristic learned to be chosen with Gen Algo

\cite{DomKnThmProv} -- inference control heuristics for equational deduction.
Data from prev proofs, select equations that are likely to be used in new
situations. 1st eval fn works by symbolic retrieval of generalized patterns
from a kkn base, 2nd eval fn compiles the knowledge into abstract term
evaluation trees. \textbf{Analyzed proof protocols} by representing knowledge
about protocols'n'proofs 

\cite{HighPerfATPAI} -- case-based reasoning, similarity concept, cooperation
concept, reactive planning; still `learn' from previous successful proof
attempts'

\cite{FeatureBasedThmP} -- Learn search-guiding heuristics by employing
features in a simple, yet effective manner. Features used to adapts a heuristic
to a solved problem. Utilize heuristic profitably for related target problems.
\textbf{Prediction of usefulness of a fact.} 

SNoW \cite{SNoW} -- learning program that can be used as a general purpose
mulit-class classifier and is specifically tailored for large number of
features. Sparse Network of Winnows (not a typo). Sparse network of sparse
linear functions over a pre-defined or incrementally acquired feature space.
Several update rules may be used -- sparse variations of the Winnow update
rule, the Perceptron or Naive Bayes. Multi class learner. Decisions either
binary or continuous (confidence in [0, 1]).

Proof General \cite{ProofGeneral} -- tool for developing proofs with ITP.
Interaction based around proof script (seq of commands sent to ITP). Provides
UI.

Mizar proof advisor \cite{MizarProofAdvisor} -- MPTP (Mizar Problems for
Theorem Proving) is system described; translates MML into FOL for ATPs and for
generating thm proving problems corresponding to Mizar Mathematical Library.
Mizar proof advisor used for selecting suitable axioms from the large library
for an arbirtrary problem. Feature based ML framework, symbols are the features
that characterise formulas. They had 40k targets and about 7k features.
\textbf{SNoW} learning architecture used mainly (NLP archit, designed for large
num of feat and targets).

MizarMode \cite{MizarMode} -- Emacs authoring environment. Code-generating
Code-Browsing Code-searching methods. Auto gen proof skeletons, semantic
browsing of articles, structured viewing, proof advice using \textbf{machinee
learning tools} like Mizar Proof Advisor. 

Lightweight relevance filtering ... \cite{LightweightPaulson} -- relevance
filtering methods, based on counting fn symbols in clauses. Signature based
relevance filter. Not exactly ML?...

The use of Data-Mining... \cite{DataMiningAFT} -- evaluate the applicability of
data-mining techniques for tactics from large corpuses of proofs. Data mine
information to find occuring patterns. Patterns are then evolved into tactics.
Variable Length Markov Models used to predict next proof step.

MaLARea \cite{MaLARea} -- simple metasystem iteratively combining deductive
Automated Reasoning tools (now the E and the SPASS ATP systems) with a machine
learning component (now the SNoW system used in the  naive  Bayesian  learning
mode). Intended use -- large theories, i.e. large num of problems which in
a consistent fashion use many axioms, lemmas, thms, etc. The system works in
cycles of thm proving followed by ML from successful proofs, using the learned
information to prune the set of available axioms for the next cycle. MPTP
challenge - 142/252. Learning could be stated as creating an assoc of some
features of the conjecture with proving methods. Features -- just symbols
appearing in them. "Proving method" -- ordering of all av axioms. Goal -- given
symbols, produces ordering of axioms, according to expected relevancy wrt the
set of symbols. Sufficiently simple to implement and quite efficient in the
first experiments with thm proving over Mizar library. Deduce, learn, loop
implemented via growing axiom set and growing timelimit policy. First try to
solve cheaply (min num axioms / most relevant, lowest timelimit). On success,
learning performed on the newly available solution and axiom/time limit dropped
to min values. No success -- increase limits. Details in paper. MLMLMLMLML SNoW
used in NB mode, bcs of speed. One training example contains all the symbols of
a solved conjecture w/ names of axioms needed. A bayes network is trained.
Easier to just relearn every time. Trained classifier is used to prune the
axiom set for the next runs -- we take all the unsolved conjectures and create
a testing example from each by taking all its symbols. The classifier run on
this printing (ordered) axioms. This is then used to select req num of axioms.
Usage of previous results exists.

SRASS \cite{SRASS} -- selection determined by semantics of the axioms and
conjecture, heuristically ordered by a syntactic relevance measure. Many
problems more solved. At each iter the process looks for a model of selected
axioms and the neg of the conjecture. If no model found, then the conjecture is
consequence. Otherwise, then an unselected axiom that is false in the model is
moved to the set of selected axioms. Newly selected axiom excludes the model
from the models of the selected axioms and neg conj, eventually leading to
a situation where there are no models of the selected axioms and the negated
conjecture. Unselected axioms selected in decreasing order of usefuleness.
Syntactic relevance score for usefulness. Direct relevance is ratio of how many
predicates and/or functors the have in common to how many they have overall.
Contextual direct relevance uses `contextual intersection'.

MaLARea SG \cite{MaLAReaSG} -- combines model-based and learning based methods
for automated reasoning in large theories. The implementation is based on
MaLARea. Extended by taking into account semantic relevance of axioms, similar
to SRASS. Combined system outperforms both. Three extensions to selection of
axioms. 1. check for countersatisfiability in runs where this is probable.
Allows for countersatisfiability precheck to detect more cases when more axioms
need to be added. 2. Use models found when a problem is found to be
countersatisfiable, as an additional criterion for computing axiom relevance.
Need to efficiently evaluate formulae in the models. 3. Extend axiom
specification using a logical criterion: the set of axioms should exclude as
many known models of the negated conjecture as possible. \textbf{Weird
combination. Review!}

MaLeCoP \cite{malecop} -- TABLEAU CALCULUS YAY!!! While in MaLARea
learning-based axiom selection is done outside unmodified theorem provers, in
MaLeCoP the learning-based selection is done inside the prover, and the
interaction between learning of knowledge and its application can be much
finer. The general design that we propose is as follows (see also Figure 1):
The theorem prover (P) should have a sufficiently fast communication channel to
a general advisor (A) that accepts queries (proof state descriptions) and
training data (characterization of the proof state together with solutions
and failures) from the prover, processes them, and replies to the prover
(advising, e.g., which clauses to choose). The advisor A also talks to external
system(s) (E). A translates the queries and information produced by P to the
formalism used by a particular E, and translates Eâ€™s guidance back to the
formalism used by P. At suitable time, A also hands over the (suitably
transformed) training data to E, so that E can update its knowledge of the
world on which its advice is based. A is free to spawn/query as many
instances/versions of Es as necessary, and A is responsible for managing the
guidance provided by them. Particular instances of Es that we have in mind are
learning systems, however we believe that the tableau setting is also suitable
for linking of SMT solvers, computer algebra systems, and all kinds of other AI
systems, probably in a more straightforward way than for the resolution-based
systems. SNoW again...

Premise selection .. and kernel methods \cite{PremiseSelection} -- This  work
develops  learning-based  premise  selection  in  two  ways.  First,a  newly
available  minimal dependency  analysis  of  existing  high-level  formal
mathematical proofs is used to build a large knowledge base of proof
dependencies,  providing  precise  data  for  ATP-based  re-verification  and
for  training premise selection algorithms.  Second, a new machine learning
algorithm for premise selection based on kernel methods is proposed and
implemented (Section 4 gives details) 0/1 features if sth appears in
conjecture. Looking for a classifier fn which, given a conjecture c, estimates
how useful p is for proving c. (still the aproach with chosing the best some
premises) Maths explained nicely.

Flyspeck \cite{Flyspeck} -- Trained on Flyspeck proofs. (HOL Light is an ITP)
The procedure implemented for HOLLight is currently a combination of the
external, internal, learning, and non-learning premise selection approaches.
This procedure assumes the common ITP situations of a large library of (also
definitional) theorems $T_i$ and their proofs $P_i$ (for definitions the proof
is empty). The proofs refer to other theorems giving rise to a partial ordering
of thms etended into their total chrono order. Procedure:
\begin{enumerate}
    \item characterisations of thms and proofs are extracted in a simple format
    \item dependency data are obtained by running ATPs on the ATP problems created from the hollight deps, i.e. tms are re-proved. Preferred (it is smaller) data. Exported as 1
    \item external premise selectors preprocess the thm characterizations and the proof deps. Multiple characterizations and proof dependencies may be used
    \item when an new conjecture is stated in hollight its characterization is extracted and sent to the pretrained first stage premise selectors.
    \item the first stage premise selectors work as rankers. For a given conjecture characterization they produce a ranking of the available theorems (premises) according to their (assumed) relevance for the conjecture.
    \item The best ranked premises are used inside hollight to produce atp problems. Several thresholds on num of included premises are used, resulting in multiple versions of the ATP problems.
    \item The ATPs are called on the problems. Some of the best ATPs run in a strategy-scheduling mode combining multiple strategies. Some of the strategies always use the SInE (i.e. local, second stage) premise selection (with different parameters) and some other strategies may to use SINE when the ATP problem is sufficiently large.
\end{enumerate}
\textbf{ML of Premise Selection}\\
All the currently used first-stage premise selectors are machine learning
algorithms trained in various ways on previous proofs. A number of machine
learning algorithms can be experimented with today, and in particular
kernel-based methods and ensemble methods have recently shown quite good
performance on smaller datasets such as MPTP2078. Scaling hard on large corpus.
So far this work uses mostly sparse implementation of a multiclass NB
classifier (SNoW again...). Several other fast incremental learning algorithms
were briefly tried -- perceptron and winnow algos (SNoW) and custom k-NN. Only
k-NN produced enough additional prediction power. 

At a given point during the library development, the training data available to
the machine learners are the proofs of the previously proved theorems in the
library. A frequently used approach to training premise selection is to
characterize each proof $P_i$ of theorem $T_i$ as a (multi)set of theorems
\{$T_{i_1}, ... T_{i_m}|T_{i_j} used in P_i$\} The training example will
consist of the input characterization (features) of $T_i$ (features) and the
output characterization of $T_i$ (labels) will be the multi set \{$T_i$\} and
the previous set. Such training examples can be tuned in various ways. For
example the output theorems may be further recursively expanded with their own
dependencies,the input features could be expanded with the features of their
definitions, various weighting schemes and similarity clusterings can be tried,
etc. This is also mostly left to future general research in premise-selection
learning. Once the machine learner is trained on a particular development state
of the library, it is tested on the next theorem T in the chronological order.
The input features are extracted from T and given to the trained learner which
then answers with a ranking of the available theorems. This ranking is given to
HOL Light, which uses it to produce ATP problems for T with varied numbers of
the best-ranked premise

Stronger automation for Flyspeck... \cite{FlyspeckFeatW} -- 2 complementary AI
methods used to improve strength of ai/atp service. First, several schemes for
frequency-based feature weighting are explored in combination with
distance-weighted k-nearest-neighbor classifier. A smaller improvement is
obtained by evolving targeted E prover strategies on two particular premise
selections, using the Blind Strategymaker (BliStr) system. (FOR THE PREVIOUS)
custom implementation of the k-nearest neighbor (k-NN) machine-learning method,
which computes for a new example (conjecture) the k nearest (in a given feature
distance) previous examples and ranks premises by their frequency in these
examples. (\\END) The  simplest  way  how  to  measure  the  similarity  of
formulas  to  the  new  conjecture  is  to compute the overlap of their
(sparse) feature vectors. Neglected  by  our  first  implementation  is
however  the  sensitivity  of k-NN  to  feature  frequencies. Inverse Document
Fequency weighting. 

MaSh \cite{MaSh} -- Sledgehammer had relevance filter (syntactic similarity).
Mash learns from succsessful proofs. Integrates easily. \emph{Draws on recent
research in the context of Mizar and HOL Light.} CUSTOM version of a weighted
sparse naiveB algo, that is faster than the NB in SNoW. Maintains persistent
state and supports incremental, nonmonotonic updates.  The main technical
difficulty is to perform the learning in a fast and robust way without
interfering with other activities of the proof assistant. Power users can
enhance the learning by letting external provers run for hours on libraries,
searching for simpler proofs. A particularly strong filter, MeSh, is obtained
by combining MePo (MEPO is paulsons thingie which selects based on num of
relevant symbols) and MaSh. Implementations refines this in several ways --
chained facts take absolutie priority, local facts are preferred to gloobal,
first order facts preferred to hol ones; rare symbols are weighted more
heavily; etc. Mepo tents to perform best on that contain some rare symbols,
otherwise it discriminates poorly. There is also issue of starvation: the
filter with its iterative expansion of the set of relevant symbols effectively
performs a best first search and may ignore some relevant facts close to the
root. Provers given ranked selected facts. Time limit and number of facts vary
(the classic setting). Once a proof is found, Sledgehammer miinimizes it by
invoking the prover repeatedly with subsets of the facts it refers to. 
\\\textbf{The ML engine}\\
Default algorithm NB adapted to fact selection. Manipulates thm proving
concepts in an abstract way. Handcrafted features. Sources of proofs -- all
facts in theories. Most interesting lemmas, those written by man. (see paper
for math details)

MaLARea04 \cite{MaLARea04} -- seems like nothing new...




\bibliographystyle{apalike}
\bibliography{refs.bib}

\end{document}
